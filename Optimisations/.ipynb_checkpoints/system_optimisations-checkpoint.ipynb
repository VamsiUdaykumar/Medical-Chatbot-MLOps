{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6ddab4-9173-4468-8fbb-f352961b2f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "####Wrapping in Fast API\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import onnxruntime as ort\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "class TextRequest(BaseModel):\n",
    "    text: str\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "session = ort.InferenceSession(\"gpt2.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(request: TextRequest):\n",
    "    input_ids = tokenizer.encode(request.text, return_tensors=\"np\").astype(np.int64)\n",
    "    logits = session.run(None, {\"input_ids\": input_ids})[0]\n",
    "    predicted_token_id = logits[0][-1].argmax()\n",
    "    response = tokenizer.decode([predicted_token_id])\n",
    "    return {\"response\": response}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f308b062-4969-4f8f-82bd-5ac916a4a7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "uvicorn app:app --host 0.0.0.0 --port 8000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b946d80b-a119-436f-b605-0fc4268502b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "######Bench mark Fast API\n",
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "FASTAPI_URL = \"http://localhost:8000/predict\"\n",
    "payload = {\"text\": \"What are the symptoms of COVID-19?\"}\n",
    "num_requests = 100\n",
    "inference_times = []\n",
    "\n",
    "for _ in range(num_requests):\n",
    "    start = time.time()\n",
    "    response = requests.post(FASTAPI_URL, json=payload)\n",
    "    end = time.time()\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        inference_times.append(end - start)\n",
    "    else:\n",
    "        print(\"Error:\", response.text)\n",
    "\n",
    "inference_times = np.array(inference_times)\n",
    "print(f\"Median latency: {np.median(inference_times)*1000:.2f} ms\")\n",
    "print(f\"95th percentile: {np.percentile(inference_times, 95)*1000:.2f} ms\")\n",
    "print(f\"99th percentile: {np.percentile(inference_times, 99)*1000:.2f} ms\")\n",
    "print(f\"Throughput: {num_requests / np.sum(inference_times):.2f} req/s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2844d82-4381-4cdf-8965-283bdf18b12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "######Concurrency TEsts\n",
    "import concurrent.futures\n",
    "\n",
    "def send_request(payload):\n",
    "    start = time.time()\n",
    "    r = requests.post(FASTAPI_URL, json=payload)\n",
    "    end = time.time()\n",
    "    return end - start if r.status_code == 200 else None\n",
    "\n",
    "def run_concurrent_tests(num_requests, max_workers):\n",
    "    inference_times = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(send_request, payload) for _ in range(num_requests)]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result: inference_times.append(result)\n",
    "    return inference_times\n",
    "\n",
    "times = run_concurrent_tests(1000, max_workers=16)\n",
    "\n",
    "print(f\"Median latency: {np.median(times)*1000:.2f} ms\")\n",
    "print(f\"Throughput: {len(times) / np.sum(times):.2f} req/s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e553a6-9e50-4e64-8042-399e77724a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimizations Lab Manual\n",
    "\n",
    "Major Phase\tSubtasks\tDescription\n",
    "1. FastAPI Endpoint\ta) Wrap model in FastAPI\n",
    "b) Launch locally\tTurn your PyTorch/ONNX model into a FastAPI server endpoint for inference\n",
    "2. Benchmarking FastAPI\ta) Single User\n",
    "b) Measure Median, 95th, 99th Latency, Throughput\tSend 100 sequential requests to FastAPI and collect metrics\n",
    "3. Concurrent Clients Stress Test\ta) Run with 16 concurrent threads\n",
    "b) Measure new latencies and throughput\tSimulate high load (queuing delay grows)\n",
    "4. Triton Inference Server Setup\ta) Serve model via Triton\n",
    "b) Docker Compose setup\tDeploy a production-grade Triton server (for GPU servers like Chameleon)\n",
    "5. Triton Dynamic Batching\ta) Enable dynamic batching in config.pbtxt\n",
    "b) Re-benchmark performance\tAllow Triton to batch requests automatically and improve efficiency\n",
    "6. Scaling on Multiple GPUs\ta) Configure multiple model instances across GPUs\tServe models across 2× GPUs (e.g., P100s)\n",
    "7. Serving ONNX Optimized Models\ta) Migrate Triton to ONNX Backend\n",
    "b) Improve serving latency\tDeploy optimized ONNX model instead of PyTorch model in Triton\n",
    "8. Benchmark All Scenarios\ta) Compare FastAPI vs Triton\n",
    "b) Compare PyTorch backend vs ONNX backend in Triton\tFull system performance comparison at different concurrency levels\n",
    "9. Update Flask App\t(only if using Flask frontend)\tUpdate your frontend app to correctly interface with Triton’s ONNX servi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d20f4a9-138f-4440-aa45-cd5d26aaaf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "######Do Triton part on gpu\n",
    "\n",
    "4️⃣ Triton Inference Server Setup (Once on GPU)\n",
    "a) Folder structure\n",
    "arduino\n",
    "Copy\n",
    "Edit\n",
    "models/\n",
    "└── gpt2_model\n",
    "    ├── 1\n",
    "    │   └── model.onnx\n",
    "    └── config.pbtxt\n",
    "b) Example config.pbtxt (for GPT-2 ONNX model)\n",
    "\n",
    "name: \"gpt2_model\"\n",
    "backend: \"onnxruntime\"\n",
    "max_batch_size: 16\n",
    "input [\n",
    "  {\n",
    "    name: \"input_ids\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [ -1, -1 ]  # flexible batch and sequence length\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"logits\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1, -1, -1 ]\n",
    "  }\n",
    "]\n",
    "instance_group [\n",
    "  {\n",
    "    count: 1\n",
    "    kind: KIND_GPU\n",
    "    gpus: [ 0 ]\n",
    "  }\n",
    "]\n",
    "\n",
    "c) Launch Triton server (on GPU)\n",
    "\n",
    "docker run --gpus all --rm -p8000:8000 -p8001:8001 -p8002:8002 \\\n",
    "-v $(pwd)/models:/models nvcr.io/nvidia/tritonserver:23.01-py3 \\\n",
    "tritonserver --model-repository=/models\n",
    "✅ Triton server will start and serve GPT-2 ONNX model.\n",
    "\n",
    "\n",
    "5️⃣ Triton Client Benchmark (Performance Analyzer)\n",
    "Inside Triton container or another container:\n",
    "\n",
    "perf_analyzer -u localhost:8000 -m gpt2_model -b 1 --input-data input.json\n",
    "✅ Reports:\n",
    "\n",
    "Queuing delay\n",
    "\n",
    "Compute infer latency\n",
    "\n",
    "Throughput\n",
    "\n",
    "6️⃣ Enable Dynamic Batching in Triton\n",
    "Edit config.pbtxt:\n",
    "\n",
    "txt\n",
    "Copy\n",
    "Edit\n",
    "dynamic_batching {\n",
    "  preferred_batch_size: [4, 8, 16]\n",
    "  max_queue_delay_microseconds: 100\n",
    "}\n",
    "✅ Then restart Triton Server and benchmark again.\n",
    "\n",
    "7️⃣ Scale Across GPUs in Triton\n",
    "Edit config.pbtxt again:\n",
    "\n",
    "txt\n",
    "Copy\n",
    "Edit\n",
    "instance_group [\n",
    "  {\n",
    "    count: 2\n",
    "    kind: KIND_GPU\n",
    "    gpus: [ 0, 1 ]\n",
    "  }\n",
    "]\n",
    "✅ Restart Triton and run performance analyzer again!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
