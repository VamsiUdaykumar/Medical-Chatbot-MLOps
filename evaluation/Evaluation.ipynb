{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60f503f4-0929-45b7-b7f1-b65d0e8d3432",
   "metadata": {},
   "source": [
    "### Prepare a Held out test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79de460-4859-4fe5-b6e5-c50b2091caaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load MedQuAD dataset from Hugging Face\n",
    "medquad = load_dataset(\"medquad\")\n",
    "\n",
    "# Splitting into training (90%) and held-out test set (10%)\n",
    "medquad = medquad['train'].train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# Save the splits\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': medquad['train'],\n",
    "    'test': medquad['test']\n",
    "})\n",
    "\n",
    "dataset_dict.save_to_disk(\"medquad_dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae9d9cc-4e38-455e-80fe-0c427daa9457",
   "metadata": {},
   "source": [
    "### Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de771203-6b9d-4afd-83d3-5ce75181440e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"your-trained-model-name\"  # Replace with your model path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Load test set\n",
    "test_set = load_from_disk(\"medquad_dataset\")[\"test\"]\n",
    "\n",
    "def generate_answer(question):\n",
    "    inputs = tokenizer.encode(question, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_length=256,\n",
    "        do_sample=False\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Generate predictions\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for example in test_set:\n",
    "    question = example['question']\n",
    "    reference_answer = example['answer']\n",
    "\n",
    "    predicted_answer = generate_answer(question)\n",
    "\n",
    "    predictions.append(predicted_answer)\n",
    "    references.append(reference_answer)\n",
    "\n",
    "# Save predictions and references\n",
    "import json\n",
    "with open(\"test_predictions.json\", \"w\") as f:\n",
    "    json.dump({\"predictions\": predictions, \"references\": references}, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183816a8-50fd-46c9-8427-c1d1001417b3",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ff6e4b-d00c-47e4-8875-ab52c52f0a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import evaluate\n",
    "\n",
    "# Load predictions and references\n",
    "with open(\"test_predictions.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "predictions = data[\"predictions\"]\n",
    "references = data[\"references\"]\n",
    "\n",
    "# Compute BLEU\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "\n",
    "# Compute ROUGE-L\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "rouge_score = rouge.compute(predictions=predictions, references=references, rouge_types=[\"rougeL\"])\n",
    "\n",
    "print(f\"BLEU Score: {bleu_score['bleu']:.4f}\")\n",
    "print(f\"ROUGE-L Score: {rouge_score['rougeL']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d3bf63-9e43-4c31-9fb6-1c3fa985a53b",
   "metadata": {},
   "source": [
    "### ML Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e15e33-5bd9-4e11-99e2-752b8225944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment(\"Medical-Chatbot-Evaluation\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_metric(\"BLEU\", bleu_score['bleu'])\n",
    "    mlflow.log_metric(\"ROUGE-L\", rouge_score['rougeL'])\n",
    "    mlflow.log_artifact(\"test_predictions.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cbe329-7c0b-4861-bddc-262b1db6c9f9",
   "metadata": {},
   "source": [
    "## Human Judgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f562fc9b-d036-4c6e-ab30-4dd5656c65c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Part 2 – Sanity Checks with Human Judgment & Explainable AI\n",
    "# ---------------------------------------------------------\n",
    "#\n",
    "# Prereqs:\n",
    "#   pip install datasets transformers torch nltk lime evaluate\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "import json, random, torch\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "# ---------- 1.  Load held‑out test set & model ----------\n",
    "\n",
    "DATA_DIR   = \"medquad_dataset\"          # from Step 1\n",
    "MODEL_NAME = \"your-trained-model-name\"  # path or HF repo id\n",
    "\n",
    "test_set  = load_from_disk(DATA_DIR)[\"test\"]\n",
    "tok       = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModelForCausalLM.from_pretrained(MODEL_NAME).eval().to(\n",
    "              torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# ---------- 2.  Generate answers on the full test set ----------\n",
    "\n",
    "def answer(q: str, max_len: int = 256) -> str:\n",
    "    ids = tok.encode(q, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(ids, max_length=max_len, do_sample=False)\n",
    "    return tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "preds, refs = [], []\n",
    "for ex in test_set:\n",
    "    preds.append(answer(ex[\"question\"]))\n",
    "    refs.append(ex[\"answer\"])\n",
    "\n",
    "with open(\"test_predictions.json\", \"w\") as f:\n",
    "    json.dump({\"predictions\": preds, \"references\": refs}, f, indent=2)\n",
    "\n",
    "# ---------- 3.  Sample ~50 Q‑A pairs for human review ----------\n",
    "\n",
    "SAMPLE_N = 50\n",
    "idxs = random.sample(range(len(preds)), SAMPLE_N)\n",
    "sampled = [{\"question\": refs[i], \"answer\": preds[i]} for i in idxs]\n",
    "\n",
    "with open(\"human_review_samples.json\", \"w\") as f:\n",
    "    json.dump(sampled, f, indent=2)\n",
    "\n",
    "# ---------- 4.  Local interpretability with LIME ----------\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=list(tok.get_vocab().keys()))\n",
    "\n",
    "def proba_func(questions: list[str]):\n",
    "    outs = []\n",
    "    for q in questions:\n",
    "        ids   = tok(q, return_tensors=\"pt\").to(model.device)\n",
    "        logits = model(**ids, labels=ids[\"input_ids\"]).logits[:, -1, :]\n",
    "        outs.append(torch.softmax(logits, dim=-1).cpu().numpy().ravel())\n",
    "    return outs\n",
    "\n",
    "sample_q = \"What are the early signs of stroke?\"\n",
    "lime_exp = explainer.explain_instance(\n",
    "    sample_q,\n",
    "    proba_func,\n",
    "    num_features=10,\n",
    "    num_samples=100\n",
    ")\n",
    "\n",
    "lime_exp.save_to_file(\"lime_explanation.html\")  # open in browser\n",
    "print(lime_exp.as_list())                       # console preview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d934fdc5-5b8e-4b1c-969f-c3cb0b647e00",
   "metadata": {},
   "source": [
    "### Template Based tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2827c8b-5417-4c58-b441-833be3be68b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Make paraphrase sets\n",
    "# -----------------------------------------------\n",
    "# make_paraphrase_sets.py\n",
    "# -----------------------------------------------\n",
    "import json, re, random\n",
    "from pathlib import Path\n",
    "from itertools import islice\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# 1️⃣  Canonical questions you care about\n",
    "SEED_QUESTIONS: dict[str, str] = {\n",
    "    \"common_cold_tx\":     \"How is the common cold treated?\",\n",
    "    \"diabetes_definition\": \"What is diabetes?\",\n",
    "    \"asthma_symptoms\":    \"What are the symptoms of asthma?\",\n",
    "    \"stroke_warning\":     \"What are the warning signs of a stroke?\",\n",
    "}\n",
    "\n",
    "# 2️⃣  Simple paraphrase helpers\n",
    "OPENING_TEMPLATES = [\n",
    "    \"How do doctors usually {}?\",\n",
    "    \"Can you explain {}?\",\n",
    "    \"Whats the standard {}?\",\n",
    "    \"Could you tell me {}?\",\n",
    "    \"Please describe {}.\",\n",
    "]\n",
    "\n",
    "VERB_REPLACEMENTS = {\n",
    "    \"treated\": [\"managed\", \"handled\", \"cared for\"],\n",
    "    \"defined\": [\"explained\", \"described\"],\n",
    "}\n",
    "\n",
    "def paraphrase(sentence: str, n: int = 3) -> list[str]:\n",
    "    \"\"\"\n",
    "    Very lightweight paraphrasing:\n",
    "      * swap verbs with synonyms\n",
    "      * vary opening phrases\n",
    "    \"\"\"\n",
    "    sentence = sentence.rstrip(\"?\")\n",
    "    words = sentence.split()\n",
    "\n",
    "    # verb‑level replacements\n",
    "    s2 = sentence\n",
    "    for verb, reps in VERB_REPLACEMENTS.items():\n",
    "        if verb in sentence:\n",
    "            s2_list = []\n",
    "            for r in reps:\n",
    "                s2_list.append(re.sub(rf\"\\b{verb}\\b\", r, sentence))\n",
    "            words += s2_list\n",
    "            break  # one replacement per Q\n",
    "\n",
    "    # opening templates\n",
    "    root = re.sub(r\"^(what|how|can you|could you|please|whats)\\s+\", \"\", sentence, flags=re.I)\n",
    "    templated = [tmpl.format(root.lower()) for tmpl in OPENING_TEMPLATES]\n",
    "\n",
    "    # choose *n* unique paraphrases\n",
    "    paraphrases = list({*words, *templated})\n",
    "    return list(islice(paraphrases, n))\n",
    "\n",
    "# 3️⃣  Build the structure\n",
    "out = []\n",
    "for qid, canonical in SEED_QUESTIONS.items():\n",
    "    out.append(\n",
    "        {\n",
    "            \"id\": qid,\n",
    "            \"canonical\": canonical,\n",
    "            \"paraphrases\": paraphrase(canonical, n=4),\n",
    "        }\n",
    "    )\n",
    "\n",
    "# 4️⃣  Write file\n",
    "Path(\"templates\").mkdir(exist_ok=True)\n",
    "out_path = Path(\"templates/paraphrase_sets.json\")\n",
    "out_path.write_text(json.dumps(out, indent=2))\n",
    "print(f\"✏️  Wrote {out_path} with {len(out)} template sets.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a838d6-8e04-418e-9c74-46f7b9b9b086",
   "metadata": {},
   "source": [
    "Run `pytest -q`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bead9e99-0cfc-44a0-8ff7-f9c6ce3fdab8",
   "metadata": {},
   "source": [
    "### Slice of Interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f925107-cbdf-42f8-8c42-92e556068da3",
   "metadata": {},
   "source": [
    "Dataset Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8f37a3-bb7b-4f4a-aa1a-b64bba6bdc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scripts/build_slice_cache.py\n",
    "from datasets import load_from_disk\n",
    "import json, numpy as np, evaluate\n",
    "\n",
    "DATA_DIR = \"medquad_dataset\"            # path to train/test split\n",
    "TEST_SPLIT = load_from_disk(DATA_DIR)[\"test\"]\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "slice_scores = {}   # mapping slice_name -> list[(pred, ref)]\n",
    "\n",
    "with open(\"test_predictions.json\") as f:       # generated in earlier step\n",
    "    preds_refs = json.load(f)\n",
    "\n",
    "for ex, pred, ref in zip(TEST_SPLIT, preds_refs[\"predictions\"], preds_refs[\"references\"]):\n",
    "    key = ex[\"question_type\"].replace(\" \", \"_\")  # normalize spaces\n",
    "    slice_scores.setdefault(key, {\"pred\": [], \"ref\": []})\n",
    "    slice_scores[key][\"pred\"].append(pred)\n",
    "    slice_scores[key][\"ref\"].append(ref)\n",
    "\n",
    "# compute BLEU per slice\n",
    "slice_bleu = {}\n",
    "for key, d in slice_scores.items():\n",
    "    slice_bleu[key] = bleu.compute(\n",
    "        predictions=d[\"pred\"],\n",
    "        references=[[r] for r in d[\"ref\"]]\n",
    "    )[\"bleu\"]\n",
    "\n",
    "# dump for pytest\n",
    "with open(\"templates/slice_bleu.json\", \"w\") as f:\n",
    "    json.dump(slice_bleu, f, indent=2)\n",
    "\n",
    "print(\"Wrote templates/slice_bleu.json with\", len(slice_bleu), \"slices\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bbffa1-bddd-43da-adb5-ed57159ca75c",
   "metadata": {},
   "source": [
    "Run pytest-q with test_slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bdaf1f8-84c2-425f-bd11-1776fb03e934",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Made a pytest suite for all the tests just make sure the json files are made and ready"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
