{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dacf2b-3f68-4114-ab27-36db4b86eb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run synthetic traffic to a canary endpoint and record latencies + status.\n",
    "python online_eval.py --host http://canary.chatbot --duration 10m --rps 20\n",
    "\"\"\"\n",
    "import asyncio, aiohttp, json, time, random, argparse, pathlib, csv\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def load_mix():\n",
    "    mix = json.load(open(\"canary_user_mix.json\"))[\"mix\"]\n",
    "    pool = {}\n",
    "    for sl in mix:\n",
    "        pool[sl[\"slice\"]] = json.load(open(f\"data/queries_{sl['slice']}.json\"))\n",
    "    return mix, pool\n",
    "\n",
    "async def worker(session, host, mix, pool, rps, end):\n",
    "    latencies, errors = [], 0\n",
    "    while datetime.utcnow() < end:\n",
    "        slice_ = random.choices([m[\"slice\"] for m in mix],\n",
    "                                weights=[m[\"weight\"] for m in mix])[0]\n",
    "        q = random.choice(pool[slice_])\n",
    "        t0 = time.perf_counter()\n",
    "        try:\n",
    "            async with session.post(f\"{host}/api/v1/answer\",\n",
    "                                    json={\"question\": q}, timeout=10) as resp:\n",
    "                await resp.text()\n",
    "                if resp.status >= 500:\n",
    "                    errors += 1\n",
    "        except Exception:\n",
    "            errors += 1\n",
    "        latencies.append((slice_, (time.perf_counter() - t0) * 1000))\n",
    "        await asyncio.sleep(1 / rps)\n",
    "    return latencies, errors\n",
    "\n",
    "async def main(args):\n",
    "    mix, pool = load_mix()\n",
    "    end = datetime.utcnow() + timedelta(seconds=args.duration)\n",
    "    latencies_all, err = [], 0\n",
    "    async with aiohttp.ClientSession() as sess:\n",
    "        tasks = [worker(sess, args.host, mix, pool, args.rps, end)\n",
    "                 for _ in range(args.concurrency)]\n",
    "        for coro in asyncio.as_completed(tasks):\n",
    "            lat, e = await coro\n",
    "            latencies_all.extend(lat)\n",
    "            err += e\n",
    "\n",
    "    # write raw CSV\n",
    "    pathlib.Path(\"results\").mkdir(exist_ok=True)\n",
    "    with open(\"results/online_latencies.csv\", \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"slice\", \"latency_ms\"])\n",
    "        w.writerows(latencies_all)\n",
    "\n",
    "    # summary\n",
    "    p95 = percentile([l for _, l in latencies_all], 95)\n",
    "    p50 = percentile([l for _, l in latencies_all], 50)\n",
    "    r_tot = len(latencies_all)\n",
    "    print(f\"Total requests={r_tot}, errors={err} ({err/r_tot:.2%})\")\n",
    "    print(f\"p50={p50:.0f} ms   p95={p95:.0f} ms\")\n",
    "\n",
    "def percentile(a, p):\n",
    "    a = sorted(a)\n",
    "    k = int(len(a) * p / 100)\n",
    "    return a[min(k, len(a)-1)]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--host\", required=True, help=\"http://canary.chatbot\")\n",
    "    ap.add_argument(\"--duration\", type=int, default=600, help=\"seconds\")\n",
    "    ap.add_argument(\"--rps\", type=int, default=20, help=\"requests per second / worker\")\n",
    "    ap.add_argument(\"--concurrency\", type=int, default=5, help=\"# of async workers\")\n",
    "    args = ap.parse_args()\n",
    "    asyncio.run(main(args))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6c379b-b4a0-4cdb-b37e-a2dabff66a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install aiohttp\n",
    "python online_eval.py --host http://canary.chatbot --duration 900 \\\n",
    "                      --rps 15 --concurrency 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdda9358-76bc-49c5-ae5e-e72313a3cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Grafana Dashboard code\n",
    "\n",
    "{\n",
    "  \"title\": \"Chatbot Canary – Online Evaluation\",\n",
    "  \"time\": { \"from\": \"now-15m\", \"to\": \"now\" },\n",
    "  \"refresh\": \"10s\",\n",
    "  \"schemaVersion\": 38,\n",
    "  \"version\": 1,\n",
    "  \"panels\": [\n",
    "    {\n",
    "      \"title\": \"Live RPS\",\n",
    "      \"type\": \"timeseries\",\n",
    "      \"gridPos\": { \"x\": 0, \"y\": 0, \"w\": 8, \"h\": 6 },\n",
    "      \"targets\": [\n",
    "        { \"expr\": \"rate(http_requests_total{handler=\\\"/api/v1/answer\\\"}[30s])\", \"refId\": \"A\" }\n",
    "      ],\n",
    "      \"datasource\": \"Prometheus\"\n",
    "    },\n",
    "    {\n",
    "      \"title\": \"Latency (p50 & p95 ms)\",\n",
    "      \"type\": \"timeseries\",\n",
    "      \"gridPos\": { \"x\": 8, \"y\": 0, \"w\": 8, \"h\": 6 },\n",
    "      \"targets\": [\n",
    "        {\n",
    "          \"expr\": \"histogram_quantile(0.5, rate(chatbot_latency_seconds_bucket[30s]))*1000\",\n",
    "          \"legendFormat\": \"p50\", \"refId\": \"A\"\n",
    "        },\n",
    "        {\n",
    "          \"expr\": \"histogram_quantile(0.95, rate(chatbot_latency_seconds_bucket[30s]))*1000\",\n",
    "          \"legendFormat\": \"p95\", \"refId\": \"B\"\n",
    "        }\n",
    "      ],\n",
    "      \"datasource\": \"Prometheus\"\n",
    "    },\n",
    "    {\n",
    "      \"title\": \"Error Rate\",\n",
    "      \"type\": \"stat\",\n",
    "      \"gridPos\": { \"x\": 16, \"y\": 0, \"w\": 8, \"h\": 6 },\n",
    "      \"targets\": [\n",
    "        {\n",
    "          \"expr\": \"rate(chatbot_errors_total[1m]) / rate(http_requests_total{handler=\\\"/api/v1/answer\\\"}[1m])\",\n",
    "          \"refId\": \"A\"\n",
    "        }\n",
    "      ],\n",
    "      \"datasource\": \"Prometheus\",\n",
    "      \"fieldConfig\": {\n",
    "        \"defaults\": {\n",
    "          \"unit\": \"percent\",\n",
    "          \"thresholds\": {\n",
    "            \"mode\": \"absolute\",\n",
    "            \"steps\": [\n",
    "              { \"color\": \"green\", \"value\": null },\n",
    "              { \"color\": \"yellow\", \"value\": 0.01 },\n",
    "              { \"color\": \"red\",    \"value\": 0.03 }\n",
    "            ]\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"title\": \"Requests by Slice (top 5)\",\n",
    "      \"type\": \"piechart\",\n",
    "      \"gridPos\": { \"x\": 0, \"y\": 6, \"w\": 12, \"h\": 7 },\n",
    "      \"options\": { \"displayLabels\": [\"percent\"], \"legend\": { \"displayMode\": \"table\" } },\n",
    "      \"targets\": [\n",
    "        {\n",
    "          \"expr\": \"sum by(slice)(rate(chatbot_slice_total[1m]))\",\n",
    "          \"legendFormat\": \"{{slice}}\", \"refId\": \"A\"\n",
    "        }\n",
    "      ],\n",
    "      \"datasource\": \"Prometheus\"\n",
    "    },\n",
    "    {\n",
    "      \"title\": \"Median Prediction Confidence\",\n",
    "      \"type\": \"timeseries\",\n",
    "      \"gridPos\": { \"x\": 12, \"y\": 6, \"w\": 12, \"h\": 7 },\n",
    "      \"targets\": [\n",
    "        {\n",
    "          \"expr\": \"histogram_quantile(0.5, rate(prediction_confidence_bucket[1m]))\",\n",
    "          \"legendFormat\": \"p50 confidence\", \"refId\": \"A\"\n",
    "        }\n",
    "      ],\n",
    "      \"datasource\": \"Prometheus\"\n",
    "    }\n",
    "  ]\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
