{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Latency (median): 19.06 ms\n",
      "95th percentile: 21.46 ms\n",
      "Throughput: 52.46 req/s\n"
     ]
    }
   ],
   "source": [
    "################Just Test#######################\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "\n",
    "input_text = \"What are the symptoms of COVID-19?\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# Warmup\n",
    "for _ in range(5):\n",
    "    _ = model(input_ids)\n",
    "\n",
    "# Measure latency\n",
    "latencies = []\n",
    "for _ in range(100):\n",
    "    start = time.time()\n",
    "    _ = model(input_ids)\n",
    "    latencies.append(time.time() - start)\n",
    "\n",
    "print(f\"Inference Latency (median): {np.percentile(latencies, 50)*1000:.2f} ms\")\n",
    "print(f\"95th percentile: {np.percentile(latencies, 95)*1000:.2f} ms\")\n",
    "print(f\"Throughput: {100/np.sum(latencies):.2f} req/s\")\n",
    "\n",
    "\n",
    "###################Still to do batch throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exported gpt2.onnx\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "\n",
    "onnx_model_path = \"models/gpt-2.onnx\"\n",
    "input_text = \"Hello\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "torch.onnx.export(\n",
    "    model, input_ids, onnx_model_path,\n",
    "    input_names=[\"input_ids\"], output_names=[\"logits\"],\n",
    "    dynamic_axes={\"input_ids\": {0: \"batch\", 1: \"sequence\"}, \"logits\": {0: \"batch\"}},\n",
    "    opset_version=13\n",
    ")\n",
    "print(\"✅ Exported gpt2.onnx\")\n",
    "\n",
    "#______________Just a test to convert ONNX, need to scale to lab level when working on docker______________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Inference Latency (median): 9.63 ms\n",
      "Throughput: 101.40 req/s\n"
     ]
    }
   ],
   "source": [
    "############Inferenece Session on CPU#########\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer\n",
    "import time\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "session = ort.InferenceSession(\"gpt2.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "def benchmark_session(session):\n",
    "\n",
    "    input_text = \"Tell me about diabetes.\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"np\").astype(np.int64)\n",
    "    \n",
    "    # Warm-up\n",
    "    for _ in range(5):\n",
    "        session.run(None, {\"input_ids\": input_ids})\n",
    "    \n",
    "    # Benchmark\n",
    "    latencies = []\n",
    "    for _ in range(100):\n",
    "        start = time.time()\n",
    "        session.run(None, {\"input_ids\": input_ids})\n",
    "        latencies.append(time.time() - start)\n",
    "    \n",
    "    print(f\"ONNX Inference Latency (median): {np.percentile(latencies, 50)*1000:.2f} ms\")\n",
    "    print(f\"Throughput: {100/np.sum(latencies):.2f} req/s\")\n",
    "\n",
    "\n",
    "benchmark_session(session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############Lots of Metrics Missing above, Just a template we'll scale on docker later################b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved optimized graph to gpt2_optimized.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-04-07 15:45:54.928811 [W:onnxruntime:, inference_session.cc:2039 Initialize] Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED and the NchwcTransformer enabled. The generated model may contain hardware specific optimizations, and should only be used in the same environment the model was optimized in.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "###########Apply Graph Optimisations#######\n",
    "\n",
    "import onnxruntime as ort\n",
    "\n",
    "sess_options = ort.SessionOptions()\n",
    "sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "sess_options.optimized_model_filepath = \"models/gpt2_optimized.onnx\"\n",
    "\n",
    "session = ort.InferenceSession(\"gpt2.onnx\", sess_options=sess_options, providers=[\"CPUExecutionProvider\"])\n",
    "print(\"✅ Saved optimized graph to gpt2_optimized.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Inference Latency (median): 8.97 ms\n",
      "Throughput: 110.01 req/s\n"
     ]
    }
   ],
   "source": [
    "###############Checking metrics for graph optimisedb\n",
    "onnx_model_path = \"models/gpt2_optimized.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\n",
    "benchmark_session(ort_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 16:14:40 [INFO] Start auto tuning.\n",
      "2025-04-07 16:14:40 [INFO] Quantize model without tuning!\n",
      "2025-04-07 16:14:40 [INFO] Quantize the model with default configuration without evaluating the model.                To perform the tuning process, please either provide an eval_func or provide an                    eval_dataloader an eval_metric.\n",
      "2025-04-07 16:14:40 [INFO] Adaptor has 5 recipes.\n",
      "2025-04-07 16:14:40 [INFO] 0 recipes specified by user.\n",
      "2025-04-07 16:14:40 [INFO] 3 recipes require future tuning.\n",
      "2025-04-07 16:14:40 [INFO] *** Initialize auto tuning\n",
      "2025-04-07 16:14:40 [INFO] {\n",
      "2025-04-07 16:14:40 [INFO]     'PostTrainingQuantConfig': {\n",
      "2025-04-07 16:14:40 [INFO]         'AccuracyCriterion': {\n",
      "2025-04-07 16:14:40 [INFO]             'criterion': 'relative',\n",
      "2025-04-07 16:14:40 [INFO]             'higher_is_better': True,\n",
      "2025-04-07 16:14:40 [INFO]             'tolerable_loss': 0.01,\n",
      "2025-04-07 16:14:40 [INFO]             'absolute': None,\n",
      "2025-04-07 16:14:40 [INFO]             'keys': <bound method AccuracyCriterion.keys of <neural_compressor.config.AccuracyCriterion object at 0x1764f2df0>>,\n",
      "2025-04-07 16:14:40 [INFO]             'relative': 0.01\n",
      "2025-04-07 16:14:40 [INFO]         },\n",
      "2025-04-07 16:14:40 [INFO]         'approach': 'post_training_dynamic_quant',\n",
      "2025-04-07 16:14:40 [INFO]         'backend': 'default',\n",
      "2025-04-07 16:14:40 [INFO]         'calibration_sampling_size': [\n",
      "2025-04-07 16:14:40 [INFO]             100\n",
      "2025-04-07 16:14:40 [INFO]         ],\n",
      "2025-04-07 16:14:40 [INFO]         'device': 'cpu',\n",
      "2025-04-07 16:14:40 [INFO]         'domain': 'auto',\n",
      "2025-04-07 16:14:40 [INFO]         'example_inputs': 'Not printed here due to large size tensors...',\n",
      "2025-04-07 16:14:40 [INFO]         'excluded_precisions': [\n",
      "2025-04-07 16:14:40 [INFO]         ],\n",
      "2025-04-07 16:14:40 [INFO]         'framework': 'onnxruntime',\n",
      "2025-04-07 16:14:40 [INFO]         'inputs': [\n",
      "2025-04-07 16:14:40 [INFO]         ],\n",
      "2025-04-07 16:14:40 [INFO]         'model_name': '',\n",
      "2025-04-07 16:14:40 [INFO]         'op_name_dict': None,\n",
      "2025-04-07 16:14:40 [INFO]         'op_type_dict': None,\n",
      "2025-04-07 16:14:40 [INFO]         'outputs': [\n",
      "2025-04-07 16:14:40 [INFO]         ],\n",
      "2025-04-07 16:14:40 [INFO]         'quant_format': 'default',\n",
      "2025-04-07 16:14:40 [INFO]         'quant_level': 'auto',\n",
      "2025-04-07 16:14:40 [INFO]         'recipes': {\n",
      "2025-04-07 16:14:40 [INFO]             'smooth_quant': False,\n",
      "2025-04-07 16:14:40 [INFO]             'smooth_quant_args': {\n",
      "2025-04-07 16:14:40 [INFO]             },\n",
      "2025-04-07 16:14:40 [INFO]             'layer_wise_quant': False,\n",
      "2025-04-07 16:14:40 [INFO]             'layer_wise_quant_args': {\n",
      "2025-04-07 16:14:40 [INFO]             },\n",
      "2025-04-07 16:14:40 [INFO]             'fast_bias_correction': False,\n",
      "2025-04-07 16:14:40 [INFO]             'weight_correction': False,\n",
      "2025-04-07 16:14:40 [INFO]             'gemm_to_matmul': True,\n",
      "2025-04-07 16:14:40 [INFO]             'graph_optimization_level': None,\n",
      "2025-04-07 16:14:40 [INFO]             'first_conv_or_matmul_quantization': True,\n",
      "2025-04-07 16:14:40 [INFO]             'last_conv_or_matmul_quantization': True,\n",
      "2025-04-07 16:14:40 [INFO]             'pre_post_process_quantization': True,\n",
      "2025-04-07 16:14:40 [INFO]             'add_qdq_pair_to_weight': False,\n",
      "2025-04-07 16:14:40 [INFO]             'optypes_to_exclude_output_quant': [\n",
      "2025-04-07 16:14:40 [INFO]             ],\n",
      "2025-04-07 16:14:40 [INFO]             'dedicated_qdq_pair': False,\n",
      "2025-04-07 16:14:40 [INFO]             'rtn_args': {\n",
      "2025-04-07 16:14:40 [INFO]             },\n",
      "2025-04-07 16:14:40 [INFO]             'awq_args': {\n",
      "2025-04-07 16:14:40 [INFO]             },\n",
      "2025-04-07 16:14:40 [INFO]             'gptq_args': {\n",
      "2025-04-07 16:14:40 [INFO]             },\n",
      "2025-04-07 16:14:40 [INFO]             'teq_args': {\n",
      "2025-04-07 16:14:40 [INFO]             },\n",
      "2025-04-07 16:14:40 [INFO]             'autoround_args': {\n",
      "2025-04-07 16:14:40 [INFO]             }\n",
      "2025-04-07 16:14:40 [INFO]         },\n",
      "2025-04-07 16:14:40 [INFO]         'reduce_range': None,\n",
      "2025-04-07 16:14:40 [INFO]         'TuningCriterion': {\n",
      "2025-04-07 16:14:40 [INFO]             'max_trials': 100,\n",
      "2025-04-07 16:14:40 [INFO]             'objective': [\n",
      "2025-04-07 16:14:40 [INFO]                 'performance'\n",
      "2025-04-07 16:14:40 [INFO]             ],\n",
      "2025-04-07 16:14:40 [INFO]             'strategy': 'basic',\n",
      "2025-04-07 16:14:40 [INFO]             'strategy_kwargs': None,\n",
      "2025-04-07 16:14:40 [INFO]             'timeout': 0\n",
      "2025-04-07 16:14:40 [INFO]         },\n",
      "2025-04-07 16:14:40 [INFO]         'use_bf16': True,\n",
      "2025-04-07 16:14:40 [INFO]         'ni_workload_name': 'quantization'\n",
      "2025-04-07 16:14:40 [INFO]     }\n",
      "2025-04-07 16:14:40 [INFO] }\n",
      "2025-04-07 16:14:40 [WARNING] [Strategy] Please install `mpi4py` correctly if using distributed tuning; otherwise, ignore this warning.\n",
      "2025-04-07 16:14:40 [WARNING] The model is automatically detected as a non-NLP model. You can use 'domain' argument in 'PostTrainingQuantConfig' to overwrite it\n",
      "2025-04-07 16:14:40 [WARNING] Graph optimization level is automatically set to ENABLE_BASIC. You can use 'recipe' argument in 'PostTrainingQuantConfig'to overwrite it\n",
      "2025-04-07 16:14:43 [INFO] Do not evaluate the baseline and quantize the model with default configuration.\n",
      "2025-04-07 16:14:43 [INFO] Quantize the model with default config.\n",
      "2025-04-07 16:14:50 [INFO] |**********Mixed Precision Statistics*********|\n",
      "2025-04-07 16:14:50 [INFO] +-----------------------+-------+------+------+\n",
      "2025-04-07 16:14:50 [INFO] |        Op Type        | Total | INT8 | FP32 |\n",
      "2025-04-07 16:14:50 [INFO] +-----------------------+-------+------+------+\n",
      "2025-04-07 16:14:50 [INFO] |         MatMul        |   73  |  49  |  24  |\n",
      "2025-04-07 16:14:50 [INFO] |         Gather        |   99  |  2   |  97  |\n",
      "2025-04-07 16:14:50 [INFO] |    DequantizeLinear   |   2   |  2   |  0   |\n",
      "2025-04-07 16:14:50 [INFO] | DynamicQuantizeLinear |   49  |  49  |  0   |\n",
      "2025-04-07 16:14:50 [INFO] +-----------------------+-------+------+------+\n",
      "2025-04-07 16:14:50 [INFO] Pass quantize model elapsed time: 6269.01 ms\n",
      "2025-04-07 16:14:50 [INFO] Save tuning history to /Users/tejdeepchippa/Desktop/All/MLops/Project/flask_gpt2/nc_workspace/2025-04-07_16-14-12/./history.snapshot.\n",
      "2025-04-07 16:14:50 [INFO] [Strategy] Found the model meets accuracy requirements, ending the tuning process.\n",
      "2025-04-07 16:14:50 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.\n",
      "2025-04-07 16:14:50 [INFO] Save deploy yaml to /Users/tejdeepchippa/Desktop/All/MLops/Project/flask_gpt2/nc_workspace/2025-04-07_16-14-12/deploy.yaml\n"
     ]
    }
   ],
   "source": [
    "import neural_compressor\n",
    "from neural_compressor import quantization\n",
    "\n",
    "# Load ONNX model into Intel Neural Compressor\n",
    "model_path = \"models/gpt2.onnx\"\n",
    "fp32_model = neural_compressor.model.onnx_model.ONNXModel(model_path)\n",
    "\n",
    "# Configure the quantizer\n",
    "config_ptq = neural_compressor.PostTrainingQuantConfig(\n",
    "    approach=\"dynamic\"\n",
    ")\n",
    "\n",
    "# Fit the quantized model\n",
    "q_model = quantization.fit(\n",
    "    model=fp32_model, \n",
    "    conf=config_ptq\n",
    ")\n",
    "\n",
    "# Save quantized model\n",
    "q_model.save_model_to_file(\"models/food11_quantized_dynamic.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################Do static quantisation on the DOCKER container#############\n",
    "################Run benchmarks on all the quantised models like before#########bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############Below things wont work now but once we have gpu they will#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Inference Latency (median): 9.14 ms\n",
      "Throughput: 108.20 req/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'CPU'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer\n",
    "import time\n",
    "onnx_model_path = \"models/gpt2.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CUDAExecutionProvider'])\n",
    "benchmark_session(ort_session)\n",
    "ort.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Inference Latency (median): 9.17 ms\n",
      "Throughput: 107.88 req/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'CPU'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_model_path = \"models/gpt2.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['TensorrtExecutionProvider'])\n",
    "benchmark_session(ort_session)\n",
    "ort.get_device()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
